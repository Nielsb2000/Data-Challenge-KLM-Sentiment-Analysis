{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_files(path_directory):\n",
    "    \"\"\"\n",
    "    Extracts all files names in a directory\n",
    "    :param path_directory: path to the directory to extract files from\n",
    "    :return: list of file paths in a given directory\n",
    "    \"\"\"\n",
    "        \n",
    "    directory = os.listdir(path_directory)\n",
    "    json_files = []\n",
    "    \n",
    "    # Append all path files to list of json_files \n",
    "\n",
    "    for file in directory:\n",
    "        if file.startswith('airlines-'):\n",
    "            json_files.append(os.path.join(path_directory, file))\n",
    "\n",
    "    return json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "def read_json(path_file: str):\n",
    "    \"\"\"\n",
    "    Reads given json file, try to read entire file at once \n",
    "    :param path_files: path of the file\n",
    "    :return: DataFrame of all tweets in given file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Path counter\n",
    "    \n",
    "    global count\n",
    "    print(str(count) + \" \" + path_file)\n",
    "    count = count + 1\n",
    "    \n",
    "    # Try to read entire file at once, dtypes for tweet id's given to avoid JSON large number inaccuracy\n",
    "\n",
    "    try:\n",
    "        df_tweet_objects = pd.read_json(path_file, dtype={'id_str': object, 'in_reply_to_user_id_str': object, \n",
    "                                                          'in_reply_to_status_id_str': object, 'quoted_status_id_str': object})\n",
    "    \n",
    "    # When an error occurs, read file line by line \n",
    "    \n",
    "    except ValueError:\n",
    "        with open(path_file) as json_file:\n",
    "            list_tweets = []\n",
    "            for json_object in json_file:\n",
    "                try:\n",
    "                    tweet = json.loads(json_object)\n",
    "                    list_tweets.append(tweet)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            df_tweet_objects = pd.DataFrame(list_tweets)\n",
    "\n",
    "    return df_tweet_objects.dropna(subset=['id_str', 'user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables(tweets):\n",
    "    \"\"\"\n",
    "    Extracts nested dictionaries and\n",
    "    :param tweets: DataFrame of all tweet information\n",
    "    :return: one DataFrame for tweets and one DataFrame for users   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Using pandas json_normalize to extract all keys from the nested user dictionary \n",
    "    \n",
    "    users = pd.json_normalize(tweets['user'])\n",
    "    \n",
    "    # For loop to extract full_text from nested extended_tweet dictionary \n",
    "\n",
    "    for index, dictionary in enumerate(tweets['extended_tweet']):\n",
    "        try:\n",
    "            full_text = dictionary.get('full_text', np.nan)\n",
    "            tweets.at[index, 'full_text'] = full_text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # For loop to extract mentioned user id's from nested entities dictionary, return as tuple of user id's\n",
    "    \n",
    "    for index, dictionary in enumerate(tweets['entities']):\n",
    "        mention_list = []\n",
    "        for record in dictionary['user_mentions']:\n",
    "            try:\n",
    "                mentions = record.get('id_str', np.nan)\n",
    "                mention_list.append(mentions)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        tweets.at[index, 'mentioned_users'] = ', '.join(filter(None, mention_list))\n",
    "    \n",
    "    tweets.rename(columns={'tweet_id': 'id_str'})\n",
    "\n",
    "    # Match tweet and user id's to other tables, 'rename' columns and add empty columns as placeholders for later analysis\n",
    "    \n",
    "    tweets['tweet_id'] = tweets['id_str']        \n",
    "    tweets['user_id'] = users['id_str']\n",
    "    tweets['screen_name'] = users['screen_name']\n",
    "    tweets['user_category'] = 'account'\n",
    "    tweets['conversation_id'] = tweets['tweet_id']\n",
    "    tweets['conversation_level'] = 0\n",
    "    tweets['tokenized_text'] = 'unknown'\n",
    "    tweets['created_at'] = pd.to_datetime(tweets['created_at'], infer_datetime_format=True)\n",
    "    \n",
    "    users['user_id'] = users['id_str']\n",
    "    users['tweet_id'] = tweets['id_str']\n",
    "    users['user_category'] = 'account'\n",
    "    users['tweet_created_at'] = tweets['created_at']\n",
    "    \n",
    "    # Clean replies, quotes and tweet text by replacing nan values by proper values \n",
    "    \n",
    "    tweets['reply_to_tweet'] = tweets['in_reply_to_status_id_str'].replace({np.nan: 0})\n",
    "    tweets['reply_to_user'] = tweets['in_reply_to_user_id_str'].replace({np.nan: 0})\n",
    "    tweets['quote_tweet'] = tweets['quoted_status_id_str'].replace({np.nan: 0})\n",
    "    tweets['full_text'] = tweets['full_text'].fillna(tweets['text'])\n",
    "    \n",
    "    return tweets, users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(tweets, users):\n",
    "    \"\"\"\n",
    "    Selects correct columns from DataFrames and cleans types\n",
    "    :param tweets: DataFrame of all tweets\n",
    "    :param users: DataFrame of all users\n",
    "    :return: cleaned DataFrames \n",
    "    \"\"\"\n",
    "    \n",
    "    # Copying useful columns into cleaned DataFrame \n",
    "    \n",
    "    clean_tweets = tweets[['tweet_id', 'user_id', 'screen_name', 'user_category', 'full_text',\n",
    "                           'tokenized_text', 'reply_to_tweet', 'reply_to_user', 'mentioned_users', \n",
    "                           'quote_tweet', 'conversation_id', 'conversation_level', 'lang', 'created_at']]\n",
    "    \n",
    "    clean_users = users[['user_id', 'tweet_id', 'name', 'screen_name', 'user_category', 'followers_count', \n",
    "                         'friends_count', 'statuses_count', 'verified', 'url', 'description', 'tweet_created_at']]\n",
    "    \n",
    "    # Defining correct types for columns in DataFrame \n",
    "    \n",
    "    tweet_dict = {'tweet_id': int, 'user_id': int, 'screen_name': str, 'user_category': str, 'full_text': str, 'tokenized_text': str, \n",
    "                  'reply_to_tweet': int, 'reply_to_user': int, 'quote_tweet': int, 'mentioned_users': str, \n",
    "                  'conversation_id': str, 'conversation_level': int, 'lang': str, 'created_at': int}\n",
    "    \n",
    "    user_dict = {'user_id': int, 'tweet_id': int, 'name': str, 'screen_name': str, 'user_category': str,\n",
    "                 'followers_count': int, 'friends_count': int, 'statuses_count': int, 'verified': bool, \n",
    "                 'url': str, 'description': str, 'tweet_created_at': int}\n",
    "    \n",
    "    clean_tweets = clean_tweets.astype(tweet_dict)\n",
    "    clean_users = clean_users.astype(user_dict)\n",
    "    \n",
    "    # Adjusting created at columns to be in seconds \n",
    "    \n",
    "    clean_tweets['created_at'] = clean_tweets['created_at'] // 1000000000\n",
    "    clean_users['tweet_created_at'] = clean_users['tweet_created_at'] // 1000000000\n",
    "\n",
    "    return clean_tweets, clean_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runtime(start, stop):\n",
    "    \"\"\"\n",
    "    Calculates running time of every process. This cell is repeated in every notebook. \n",
    "    :param start: start time\n",
    "    :param stop: stop time\n",
    "    :return: output corresponding to running time for process \n",
    "    \"\"\"\n",
    "    \n",
    "    runtime = stop - start\n",
    "    \n",
    "    days, seconds = runtime.days, runtime.seconds\n",
    "    \n",
    "    hours = days * 24 + seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = (seconds % 60)\n",
    "    \n",
    "    return 'Runtime loading {} files was: {} h, {} m {} s'.format(count, hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading json files into database\n",
    "\n",
    "Using seperate folders allows for easier monitoring of the loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_tweets = sqlite3.connect('tweets_airlines.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files are loaded in batches to reduce running time. From batch, a dataframe is created. This dateframe is then normalized, all useless columns are deleted and every column is given the correct type. \n",
    "\n",
    "If it is preferred to run all files at ones, only run the cells corresponding to data folder 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./data/airlines-1558744391657.json\n",
      "1 ./data/airlines-1558888297881.json\n",
      "2 ./data/airlines-1558527599826.json\n",
      "3 ./data/airlines-1558779687636.json\n",
      "4 ./data/airlines-1558697205154.json\n",
      "5 ./data/airlines-1558863520888.json\n",
      "6 ./data/airlines-1558974571041.json\n",
      "7 ./data/airlines-1558611772040.json\n",
      "8 ./data/airlines-1558623303180.json\n",
      "9 ./data/airlines-1558546003827.json\n",
      "10 ./data/airlines-1558678330070.json\n",
      "11 ./data/airlines-1558998029487.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ./data_2/airlines-1559156713030.json\n",
      "13 ./data_2/airlines-1559352498975.json\n",
      "14 ./data_2/airlines-1559097885748.json\n",
      "15 ./data_2/airlines-1559142170524.json\n",
      "16 ./data_2/airlines-1559323725358.json\n",
      "17 ./data_2/airlines-1559069822287.json\n",
      "18 ./data_2/airlines-1559231904332.json\n",
      "19 ./data_2/airlines-1559464285341.json\n",
      "20 ./data_2/airlines-1559501630259.json\n",
      "21 ./data_2/airlines-1559427581830.json\n",
      "22 ./data_2/airlines-1559256655329.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ./data_3/airlines-1559640222186.json\n",
      "24 ./data_3/airlines-1559556316966.json\n",
      "25 ./data_3/airlines-1559513326649.json\n",
      "26 ./data_3/airlines-1559769009949.json\n",
      "27 ./data_3/airlines-1559705236313.json\n",
      "28 ./data_3/airlines-1559753967610.json\n",
      "29 ./data_3/airlines-1559592585772.json\n",
      "30 ./data_3/airlines-1559549054414.json\n",
      "31 ./data_3/airlines-1559580503906.json\n",
      "32 ./data_3/airlines-1559655706075.json\n",
      "33 ./data_3/airlines-1559729883533.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 ./data_4/airlines-1559846508795.json\n",
      "35 ./data_4/airlines-1559897122639.json\n",
      "36 ./data_4/airlines-1559911151286.json\n",
      "37 ./data_4/airlines-1559902121479.json\n",
      "38 ./data_4/airlines-1559813570976.json\n",
      "39 ./data_4/airlines-1559893511277.json\n",
      "40 ./data_4/airlines-1559915161616.json\n",
      "41 ./data_4/airlines-1559909155467.json\n",
      "42 ./data_4/airlines-1559860041436.json\n",
      "43 ./data_4/airlines-1559904272004.json\n",
      "44 ./data_4/airlines-1559802437924.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 ./data_5/airlines-1559939261270.json\n",
      "46 ./data_5/airlines-1559928540833.json\n",
      "47 ./data_5/airlines-1559937665320.json\n",
      "48 ./data_5/airlines-1559926788058.json\n",
      "49 ./data_5/airlines-1559921160419.json\n",
      "50 ./data_5/airlines-1559932564771.json\n",
      "51 ./data_5/airlines-1559942816001.json\n",
      "52 ./data_5/airlines-1559934119165.json\n",
      "53 ./data_5/airlines-1559923002644.json\n",
      "54 ./data_5/airlines-1559944147286.json\n",
      "55 ./data_5/airlines-1559916992678.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 ./data_6/airlines-1559960873940.json\n",
      "57 ./data_6/airlines-1559953866862.json\n",
      "58 ./data_6/airlines-1559979261665.json\n",
      "59 ./data_6/airlines-1559988476546.json\n",
      "60 ./data_6/airlines-1559963157167.json\n",
      "61 ./data_6/airlines-1559956008864.json\n",
      "62 ./data_6/airlines-1559972279659.json\n",
      "63 ./data_6/airlines-1559969180413.json\n",
      "64 ./data_6/airlines-1559981995387.json\n",
      "65 ./data_6/airlines-1559949250686.json\n",
      "66 ./data_6/airlines-1559947622068.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 ./data_7/airlines-1559991535440.json\n",
      "68 ./data_7/airlines-1560020476933.json\n",
      "69 ./data_7/airlines-1560033347669.json\n",
      "70 ./data_7/airlines-1560000901765.json\n",
      "71 ./data_7/airlines-1560023189725.json\n",
      "72 ./data_7/airlines-1559998347395.json\n",
      "73 ./data_7/airlines-1560007831474.json\n",
      "74 ./data_7/airlines-1560014686248.json\n",
      "75 ./data_7/airlines-1560005733536.json\n",
      "76 ./data_7/airlines-1560030119237.json\n",
      "77 ./data_7/airlines-1560012550556.json\n"
     ]
    }
   ],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_8/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-00912bae4cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data_8/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-cbe8593bf2e9>\u001b[0m in \u001b[0;36mpath_files\u001b[0;34m(path_directory)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mjson_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_8/'"
     ]
    }
   ],
   "source": [
    "files = path_files('./data_8/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_9/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Folder 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = path_files('./data_10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objects = pd.concat([read_json(i) for i in files], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets, df_users = tables(df_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean, df_users_clean = select(df_tweets, df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_clean.to_sql('TWEETS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean.to_sql('USERS', conn_tweets, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_tweets.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime of Loading process for 78 files was: 0 h, 47 m 3 s'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_runtime(start, stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
